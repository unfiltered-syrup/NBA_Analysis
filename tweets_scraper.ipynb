{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = ['atlanta_hawks', 'boston_celtics', 'brooklyn_nets', 'charlotte_hornets',\n",
    "         'chicago_bulls', 'cleveland_cavaliers', 'detroit_pistons', 'indiana_pacers', 'toronto_raptors', 'dallas_mavericks', 'denver_nuggets',\n",
    "         'miami_heat', 'milwaukee_bucks', 'new_york_knicks', 'orlando_magic', 'philadelphia_76ers', 'washington_wizards', 'golden_state_warriors', 'houston_rockets',\n",
    "         'los_angeles_clippers', 'los_angeles_lakers',\n",
    "          'memphis_grizzlies', 'minnesota_timberwolves', 'new_orleans_pelicans', 'oklahoma_city_thunder', 'phoenix_suns', 'portland_trail_blazers', 'sacramento_kings'\n",
    "          , 'san_antonio_spurs', 'utah_jazz']\n",
    "\n",
    "for team in teams: #get for all teams\n",
    "    service = Service() #initialize the service\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    url = \"https://hoopshype.com/social/tweets/\"+team+\"/\"\n",
    "    driver.get(url)\n",
    "    scroll_time = 8\n",
    "    start_time = time.time()\n",
    "\n",
    "    #scroll the page to load more tweets\n",
    "    while time.time() - start_time < scroll_time:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollBy(0, 400);\")\n",
    "        # Pause for a moment to allow the page to load\n",
    "        time.sleep(1)    \n",
    "\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    articles = soup.find_all('article', class_='social-item block big')\n",
    "    p_tags = []\n",
    "\n",
    "    for article in articles:\n",
    "        p_tag = article.find('p')\n",
    "        if p_tag:\n",
    "            p_tags.append(p_tag.text)\n",
    "\n",
    "    df = pd.DataFrame(p_tags, columns=['Content'])\n",
    "    df.to_csv('twitter_data/tweets_'+ team +'.csv', index=False)\n",
    "    time.sleep(random.randint(1, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
